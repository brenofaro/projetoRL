{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision gymnasium[atari] numpy scikit-image ale_py tensorboard "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente CNN com DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import random\n",
    "from collections import deque\n",
    "import math\n",
    "from skimage.transform import resize\n",
    "from torch.utils.tensorboard import SummaryWriter  \n",
    "import os\n",
    "\n",
    "# imprimir se a gpu esta disponivel\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(input_shape), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def feature_size(self, input_shape):\n",
    "        return self.conv(torch.zeros(1, *input_shape)).view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (torch.FloatTensor(np.array(states)),\n",
    "                torch.LongTensor(np.array(actions)),\n",
    "                torch.FloatTensor(np.array(rewards)),\n",
    "                torch.FloatTensor(np.array(next_states)),\n",
    "                torch.FloatTensor(np.array(dones)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_shape, num_actions, learning_rate=0.00025, gamma=0.99, epsilon=1.0, epsilon_min=0.1, epsilon_decay=0.9999, batch_size=32, memory_size=50000, update_target_freq=1000):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = DQN(state_shape, num_actions).to(self.device)\n",
    "        self.target_model = DQN(state_shape, num_actions).to(self.device)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())  # Inicializa o target model com os mesmos pesos\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.memory = ReplayBuffer(memory_size)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.update_target_freq = update_target_freq\n",
    "        self.train_step = 0\n",
    "        self.num_actions = num_actions\n",
    "        self.loss_fn = nn.MSELoss() # Salva o Loss\n",
    "        self.episode_q_values = []  # Para a média dos Q-values\n",
    "\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randrange(self.num_actions)\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.model(state)\n",
    "            self.episode_q_values.append(q_values.mean().item())  #  Salva o Q-value\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return None, None\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.unsqueeze(1).to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "\n",
    "        q_values = self.model(states).gather(1, actions)\n",
    "        next_q_values = self.target_model(next_states).max(1)[0].unsqueeze(1)\n",
    "        target_q_values = rewards.unsqueeze(1) + self.gamma * next_q_values * (1 - dones.unsqueeze(1))\n",
    "\n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.train_step += 1\n",
    "        if self.train_step % self.update_target_freq == 0:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        return loss.item(), q_values.mean().item()\n",
    "\n",
    "    def save(self, filepath):\n",
    "        torch.save(self.model.state_dict(), filepath)\n",
    "\n",
    "def preprocess_state(state):\n",
    "    state = resize(state, (84, 84), anti_aliasing=True)\n",
    "    state = state.astype(np.float32)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def create_writer(log_metrics=False, log_dir=\"tensorboardteste/dqn_training\"):\n",
    "    return SummaryWriter(log_dir) if log_metrics else None\n",
    "\n",
    "def train_agent(env, agent, episodes=1000, save_freq=20, log_dir=\"tensorboardteste/dqn_training\", log_metrics=False):\n",
    "    \"\"\"\n",
    "    Treina o agente DQN e registra métricas no TensorBoard.\n",
    "\n",
    "    Args:\n",
    "        env: Ambiente Gym.\n",
    "        agent: Agente DQN.\n",
    "        episodes: Número de episódios para treinamento.\n",
    "        save_freq: Frequência para salvar o modelo.\n",
    "        log_dir: Diretório para salvar os logs do TensorBoard.\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        writer = create_writer (log_metrics=log_metrics, log_dir=log_dir)\n",
    "        total_steps = 0\n",
    "        running_rewards = deque(maxlen=100) # Salvar as recompensas dos ultimos 100 episodios\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            state, _ = env.reset()\n",
    "            state = preprocess_state(state)\n",
    "\n",
    "            # Frame Stacking\n",
    "            state = np.transpose(state, (2, 0, 1))\n",
    "            stacked_frames = deque([state] * 4, maxlen=4)\n",
    "            state = np.concatenate(stacked_frames, axis=0)\n",
    "\n",
    "            episode_reward = 0\n",
    "            episode_steps = 0\n",
    "            episode_start_time = time.time()\n",
    "            done = False\n",
    "            episode_actions = []  # Salvar ações do episódio\n",
    "            episode_losses = []  # Salvar o loss do episódio\n",
    "\n",
    "\n",
    "            while not done:\n",
    "                action = agent.choose_action(state)\n",
    "                episode_actions.append(action)  # Salvar ação\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                next_state = preprocess_state(next_state)\n",
    "\n",
    "                # Frame Stacking - Update\n",
    "                next_state = np.transpose(next_state, (2, 0, 1))\n",
    "                stacked_frames.append(next_state)\n",
    "                next_state = np.concatenate(stacked_frames, axis=0)\n",
    "\n",
    "                done = terminated or truncated\n",
    "                agent.memory.push(state, action, reward, next_state, done)\n",
    "                loss, mean_q = agent.learn()\n",
    "\n",
    "                if loss is not None:\n",
    "                    episode_losses.append(loss) # Salvar loss\n",
    "                    if writer:\n",
    "                        writer.add_scalar(\"Loss/step\", loss, total_steps)  # Salvar a loss no tensorboard\n",
    "                        writer.add_scalar(\"Mean_Q/step\", mean_q, total_steps) # Salvar a media dos q_values no tensorboard\n",
    "\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                episode_steps += 1\n",
    "                total_steps += 1\n",
    "\n",
    "\n",
    "            # Calculo das métricas do episódio\n",
    "            episode_duration = time.time() - episode_start_time\n",
    "            steps_per_second = episode_steps / episode_duration\n",
    "            running_rewards.append(episode_reward)\n",
    "            running_mean_reward = np.mean(running_rewards)\n",
    "            mean_reward_per_step = episode_reward / episode_steps if episode_steps > 0 else 0.0\n",
    "\n",
    "             # Calcular a ação média\n",
    "            action_counts = np.bincount(episode_actions, minlength=agent.num_actions)\n",
    "            avg_action = np.average(np.arange(agent.num_actions), weights=action_counts)\n",
    "            if writer:\n",
    "                # Salvar as métricas no tensorboard\n",
    "                writer.add_scalar(\"Episode_Reward/episode\", episode_reward, episode)\n",
    "                writer.add_scalar(\"Running_Mean_Reward/episode\", running_mean_reward, episode)\n",
    "                writer.add_scalar(\"Mean_Reward_Per_Step/episode\", mean_reward_per_step, episode)\n",
    "                writer.add_scalar(\"Episode_Duration/episode\", episode_duration, episode)\n",
    "                writer.add_scalar(\"Steps_Per_Second/episode\", steps_per_second, episode)\n",
    "                writer.add_scalar(\"Average_Action/episode\", avg_action, episode) #colocar no tensorboard a media de acoes\n",
    "                writer.add_scalar(\"Epsilon/episode\", agent.epsilon, episode)\n",
    "                if episode_losses: \n",
    "                    writer.add_scalar(\"Mean_Loss/episode\", np.mean(episode_losses), episode) # Salvar a loss media no tensorboard\n",
    "            if agent.episode_q_values: # Salvar os q_values medios\n",
    "                if writer:\n",
    "                    writer.add_scalar(\"Mean_Q_Values/episode\", np.mean(agent.episode_q_values), episode)\n",
    "                agent.episode_q_values = [] # Resetar a lista de q_values\n",
    "            \n",
    "            print(f\"{total_steps}/{1000000}: episódio: {episode + 1}, duração: {episode_duration:.3f}s, passos no episódio: {episode_steps}, passos por segundo: {steps_per_second:.1f}, recompensa do episódio: {episode_reward:.3f}, recompensa média por passo: {mean_reward_per_step:.3f}, ação média: {avg_action:.3f}, epsilon: {agent.epsilon:.6f}, running_mean_reward: {running_mean_reward:.6f}\")\n",
    "\n",
    "\n",
    "            if (episode + 1) % save_freq == 0:\n",
    "                agent.save(f'dqn_model_episode_{episode + 1}.pth')\n",
    "        if writer:\n",
    "            writer.close()\n",
    "        return agent\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rodando o treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rodando o treinamento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ale_py import ALEInterface\n",
    "from torchsummary import summary\n",
    "\n",
    "ale = ALEInterface()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('ALE/SpaceInvaders-v5', render_mode=\"rgb_array\")\n",
    "    state_shape = (12, 84, 84)\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(state_shape, num_actions)\n",
    "    # Resumo do modelo\n",
    "    summary(agent.model, state_shape, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_agent(env, agent, episodes=100000, log_metrics=False)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rodando o modelo treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from skimage.transform import resize\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(input_shape), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def feature_size(self, input_shape):\n",
    "        return self.conv(torch.zeros(1, *input_shape)).view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, state_shape, num_actions, epsilon=0.05):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = DQN(state_shape, num_actions).to(self.device)\n",
    "        self.epsilon = epsilon\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def load(self, filepath):\n",
    "        self.model.load_state_dict(torch.load(filepath, map_location=self.device))\n",
    "        self.model.eval()\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.random() < self.epsilon: # Escolha aleatória\n",
    "            return random.randrange(self.num_actions)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                q_values = self.model(state)\n",
    "                return q_values.argmax().item()\n",
    "\n",
    "def preprocess_state(state):\n",
    "    \"\"\"Resize and normalize the RGB state.\"\"\"\n",
    "    state = resize(state, (84, 84), anti_aliasing=True)\n",
    "    state = state.astype(np.float32)\n",
    "    return state\n",
    "\n",
    "def run_agent(env, agent, model_path, episodes=10):\n",
    "    agent.load(model_path)  # Carregar o modelo treinado\n",
    "    total_rewards = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = preprocess_state(state)\n",
    "\n",
    "        # Frame Stacking (RGB)\n",
    "        state = np.transpose(state, (2, 0, 1))  # (H, W, C) -> (C, H, W)\n",
    "        stacked_frames = deque([state] * 4, maxlen=4)\n",
    "        state = np.concatenate(stacked_frames, axis=0)\n",
    "\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = preprocess_state(next_state)\n",
    "\n",
    "            next_state = np.transpose(next_state, (2, 0, 1))\n",
    "            stacked_frames.append(next_state)\n",
    "            state = np.concatenate(stacked_frames, axis=0)\n",
    "\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            episode_steps += 1\n",
    "\n",
    "            env.render()\n",
    "\n",
    "\n",
    "        episode_time = time.time() - start_time\n",
    "        steps_per_second = episode_steps / episode_time\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Reward: {episode_reward}, Steps: {episode_steps}, Time: {episode_time:.2f}s, Steps/s: {steps_per_second:.2f}\")\n",
    "\n",
    "    env.close()\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    std_reward = np.std(total_rewards)\n",
    "    print(f\"\\nAverage Reward over {episodes} episodes: {avg_reward:.2f}\")\n",
    "    print(f\"Standard Deviation of Rewards: {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ale_py import ALEInterface\n",
    "\n",
    "ale = ALEInterface()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('ALE/SpaceInvaders-v5', render_mode=\"human\")\n",
    "    state_shape = (12, 84, 84) \n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    agent = Agent(state_shape, num_actions)\n",
    "    model_file = \"dqn_model_episode_2440.pth\" # Carregar o modelo treinado\n",
    "    run_agent(env, agent, model_file, episodes=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projetoiadataset",
   "language": "python",
   "name": "projetoiadataset"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
