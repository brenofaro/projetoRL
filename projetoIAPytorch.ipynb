{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\breno\\miniconda3\\envs\\projetoiadataset\\lib\\site-packages (2.6.0+cu126)\n",
      "Requirement already satisfied: torchvision in c:\\users\\breno\\miniconda3\\envs\\projetoiadataset\\lib\\site-packages (0.21.0+cu126)\n",
      "Requirement already satisfied: numpy in c:\\users\\breno\\miniconda3\\envs\\projetoiadataset\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\breno\\miniconda3\\envs\\projetoiadataset\\lib\\site-packages (0.25.2)\n",
      "Requirement already satisfied: torchsummary in c:\\users\\breno\\miniconda3\\envs\\projetoiadataset\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: ale_py in c:\\users\\breno\\miniconda3\\envs\\projetoiadataset\\lib\\site-packages (0.10.2)\n",
      "Requirement already satisfied: gymnasium[accept-rom-license,atari] in c:\\users\\breno\\miniconda3\\envs\\projetoiadataset\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\breno\\miniconda3\\envs\\projetoiadataset\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\breno\\miniconda3\\envs\\projetoiadataset\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\breno\\miniconda3\\envs\\projetoiadataset\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\breno\\miniconda3\\envs\\projetoiadataset\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\breno\\miniconda3\\envs\\projetoiadataset\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\breno\\miniconda3\\envs\\projetoiadataset\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\breno\\miniconda3\\envs\\projetoiadataset\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\breno\\miniconda3\\envs\\projetoiadataset\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\breno\\miniconda3\\envs\\projetoiadataset\\lib\\site-packages (from gymnasium[accept-rom-license,atari]) (3.1.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\breno\\miniconda3\\envs\\projetoiadataset\\lib\\site-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
      "Requirement already satisfied: scipy>=1.11.4 in c:\\users\\breno\\miniconda3\\envs\\projetoiadataset\\lib\\site-packages (from scikit-image) (1.15.2)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in c:\\users\\breno\\miniconda3\\envs\\projetoiadataset\\lib\\site-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\breno\\miniconda3\\envs\\projetoiadataset\\lib\\site-packages (from scikit-image) (2025.2.18)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\breno\\miniconda3\\envs\\projetoiadataset\\lib\\site-packages (from scikit-image) (24.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\breno\\miniconda3\\envs\\projetoiadataset\\lib\\site-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\breno\\miniconda3\\envs\\projetoiadataset\\lib\\site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: gymnasium 1.0.0 does not provide the extra 'accept-rom-license'\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision gymnasium[atari] numpy scikit-image ale_py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escala RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['CartPole-v0', 'CartPole-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Pendulum-v1', 'Acrobot-v1', 'phys2d/CartPole-v0', 'phys2d/CartPole-v1', 'phys2d/Pendulum-v0', 'LunarLander-v3', 'LunarLanderContinuous-v3', 'BipedalWalker-v3', 'BipedalWalkerHardcore-v3', 'CarRacing-v3', 'Blackjack-v1', 'FrozenLake-v1', 'FrozenLake8x8-v1', 'CliffWalking-v0', 'Taxi-v3', 'tabular/Blackjack-v0', 'tabular/CliffWalking-v0', 'Reacher-v2', 'Reacher-v4', 'Reacher-v5', 'Pusher-v2', 'Pusher-v4', 'Pusher-v5', 'InvertedPendulum-v2', 'InvertedPendulum-v4', 'InvertedPendulum-v5', 'InvertedDoublePendulum-v2', 'InvertedDoublePendulum-v4', 'InvertedDoublePendulum-v5', 'HalfCheetah-v2', 'HalfCheetah-v3', 'HalfCheetah-v4', 'HalfCheetah-v5', 'Hopper-v2', 'Hopper-v3', 'Hopper-v4', 'Hopper-v5', 'Swimmer-v2', 'Swimmer-v3', 'Swimmer-v4', 'Swimmer-v5', 'Walker2d-v2', 'Walker2d-v3', 'Walker2d-v4', 'Walker2d-v5', 'Ant-v2', 'Ant-v3', 'Ant-v4', 'Ant-v5', 'Humanoid-v2', 'Humanoid-v3', 'Humanoid-v4', 'Humanoid-v5', 'HumanoidStandup-v2', 'HumanoidStandup-v4', 'HumanoidStandup-v5', 'GymV21Environment-v0', 'GymV26Environment-v0', 'Adventure-v0', 'AdventureDeterministic-v0', 'AdventureNoFrameskip-v0', 'Adventure-v4', 'AdventureDeterministic-v4', 'AdventureNoFrameskip-v4', 'Adventure-ram-v0', 'Adventure-ramDeterministic-v0', 'Adventure-ramNoFrameskip-v0', 'Adventure-ram-v4', 'Adventure-ramDeterministic-v4', 'Adventure-ramNoFrameskip-v4', 'AirRaid-v0', 'AirRaidDeterministic-v0', 'AirRaidNoFrameskip-v0', 'AirRaid-v4', 'AirRaidDeterministic-v4', 'AirRaidNoFrameskip-v4', 'AirRaid-ram-v0', 'AirRaid-ramDeterministic-v0', 'AirRaid-ramNoFrameskip-v0', 'AirRaid-ram-v4', 'AirRaid-ramDeterministic-v4', 'AirRaid-ramNoFrameskip-v4', 'Alien-v0', 'AlienDeterministic-v0', 'AlienNoFrameskip-v0', 'Alien-v4', 'AlienDeterministic-v4', 'AlienNoFrameskip-v4', 'Alien-ram-v0', 'Alien-ramDeterministic-v0', 'Alien-ramNoFrameskip-v0', 'Alien-ram-v4', 'Alien-ramDeterministic-v4', 'Alien-ramNoFrameskip-v4', 'Amidar-v0', 'AmidarDeterministic-v0', 'AmidarNoFrameskip-v0', 'Amidar-v4', 'AmidarDeterministic-v4', 'AmidarNoFrameskip-v4', 'Amidar-ram-v0', 'Amidar-ramDeterministic-v0', 'Amidar-ramNoFrameskip-v0', 'Amidar-ram-v4', 'Amidar-ramDeterministic-v4', 'Amidar-ramNoFrameskip-v4', 'Assault-v0', 'AssaultDeterministic-v0', 'AssaultNoFrameskip-v0', 'Assault-v4', 'AssaultDeterministic-v4', 'AssaultNoFrameskip-v4', 'Assault-ram-v0', 'Assault-ramDeterministic-v0', 'Assault-ramNoFrameskip-v0', 'Assault-ram-v4', 'Assault-ramDeterministic-v4', 'Assault-ramNoFrameskip-v4', 'Asterix-v0', 'AsterixDeterministic-v0', 'AsterixNoFrameskip-v0', 'Asterix-v4', 'AsterixDeterministic-v4', 'AsterixNoFrameskip-v4', 'Asterix-ram-v0', 'Asterix-ramDeterministic-v0', 'Asterix-ramNoFrameskip-v0', 'Asterix-ram-v4', 'Asterix-ramDeterministic-v4', 'Asterix-ramNoFrameskip-v4', 'Asteroids-v0', 'AsteroidsDeterministic-v0', 'AsteroidsNoFrameskip-v0', 'Asteroids-v4', 'AsteroidsDeterministic-v4', 'AsteroidsNoFrameskip-v4', 'Asteroids-ram-v0', 'Asteroids-ramDeterministic-v0', 'Asteroids-ramNoFrameskip-v0', 'Asteroids-ram-v4', 'Asteroids-ramDeterministic-v4', 'Asteroids-ramNoFrameskip-v4', 'Atlantis-v0', 'AtlantisDeterministic-v0', 'AtlantisNoFrameskip-v0', 'Atlantis-v4', 'AtlantisDeterministic-v4', 'AtlantisNoFrameskip-v4', 'Atlantis-ram-v0', 'Atlantis-ramDeterministic-v0', 'Atlantis-ramNoFrameskip-v0', 'Atlantis-ram-v4', 'Atlantis-ramDeterministic-v4', 'Atlantis-ramNoFrameskip-v4', 'BankHeist-v0', 'BankHeistDeterministic-v0', 'BankHeistNoFrameskip-v0', 'BankHeist-v4', 'BankHeistDeterministic-v4', 'BankHeistNoFrameskip-v4', 'BankHeist-ram-v0', 'BankHeist-ramDeterministic-v0', 'BankHeist-ramNoFrameskip-v0', 'BankHeist-ram-v4', 'BankHeist-ramDeterministic-v4', 'BankHeist-ramNoFrameskip-v4', 'BattleZone-v0', 'BattleZoneDeterministic-v0', 'BattleZoneNoFrameskip-v0', 'BattleZone-v4', 'BattleZoneDeterministic-v4', 'BattleZoneNoFrameskip-v4', 'BattleZone-ram-v0', 'BattleZone-ramDeterministic-v0', 'BattleZone-ramNoFrameskip-v0', 'BattleZone-ram-v4', 'BattleZone-ramDeterministic-v4', 'BattleZone-ramNoFrameskip-v4', 'BeamRider-v0', 'BeamRiderDeterministic-v0', 'BeamRiderNoFrameskip-v0', 'BeamRider-v4', 'BeamRiderDeterministic-v4', 'BeamRiderNoFrameskip-v4', 'BeamRider-ram-v0', 'BeamRider-ramDeterministic-v0', 'BeamRider-ramNoFrameskip-v0', 'BeamRider-ram-v4', 'BeamRider-ramDeterministic-v4', 'BeamRider-ramNoFrameskip-v4', 'Berzerk-v0', 'BerzerkDeterministic-v0', 'BerzerkNoFrameskip-v0', 'Berzerk-v4', 'BerzerkDeterministic-v4', 'BerzerkNoFrameskip-v4', 'Berzerk-ram-v0', 'Berzerk-ramDeterministic-v0', 'Berzerk-ramNoFrameskip-v0', 'Berzerk-ram-v4', 'Berzerk-ramDeterministic-v4', 'Berzerk-ramNoFrameskip-v4', 'Bowling-v0', 'BowlingDeterministic-v0', 'BowlingNoFrameskip-v0', 'Bowling-v4', 'BowlingDeterministic-v4', 'BowlingNoFrameskip-v4', 'Bowling-ram-v0', 'Bowling-ramDeterministic-v0', 'Bowling-ramNoFrameskip-v0', 'Bowling-ram-v4', 'Bowling-ramDeterministic-v4', 'Bowling-ramNoFrameskip-v4', 'Boxing-v0', 'BoxingDeterministic-v0', 'BoxingNoFrameskip-v0', 'Boxing-v4', 'BoxingDeterministic-v4', 'BoxingNoFrameskip-v4', 'Boxing-ram-v0', 'Boxing-ramDeterministic-v0', 'Boxing-ramNoFrameskip-v0', 'Boxing-ram-v4', 'Boxing-ramDeterministic-v4', 'Boxing-ramNoFrameskip-v4', 'Breakout-v0', 'BreakoutDeterministic-v0', 'BreakoutNoFrameskip-v0', 'Breakout-v4', 'BreakoutDeterministic-v4', 'BreakoutNoFrameskip-v4', 'Breakout-ram-v0', 'Breakout-ramDeterministic-v0', 'Breakout-ramNoFrameskip-v0', 'Breakout-ram-v4', 'Breakout-ramDeterministic-v4', 'Breakout-ramNoFrameskip-v4', 'Carnival-v0', 'CarnivalDeterministic-v0', 'CarnivalNoFrameskip-v0', 'Carnival-v4', 'CarnivalDeterministic-v4', 'CarnivalNoFrameskip-v4', 'Carnival-ram-v0', 'Carnival-ramDeterministic-v0', 'Carnival-ramNoFrameskip-v0', 'Carnival-ram-v4', 'Carnival-ramDeterministic-v4', 'Carnival-ramNoFrameskip-v4', 'Centipede-v0', 'CentipedeDeterministic-v0', 'CentipedeNoFrameskip-v0', 'Centipede-v4', 'CentipedeDeterministic-v4', 'CentipedeNoFrameskip-v4', 'Centipede-ram-v0', 'Centipede-ramDeterministic-v0', 'Centipede-ramNoFrameskip-v0', 'Centipede-ram-v4', 'Centipede-ramDeterministic-v4', 'Centipede-ramNoFrameskip-v4', 'ChopperCommand-v0', 'ChopperCommandDeterministic-v0', 'ChopperCommandNoFrameskip-v0', 'ChopperCommand-v4', 'ChopperCommandDeterministic-v4', 'ChopperCommandNoFrameskip-v4', 'ChopperCommand-ram-v0', 'ChopperCommand-ramDeterministic-v0', 'ChopperCommand-ramNoFrameskip-v0', 'ChopperCommand-ram-v4', 'ChopperCommand-ramDeterministic-v4', 'ChopperCommand-ramNoFrameskip-v4', 'CrazyClimber-v0', 'CrazyClimberDeterministic-v0', 'CrazyClimberNoFrameskip-v0', 'CrazyClimber-v4', 'CrazyClimberDeterministic-v4', 'CrazyClimberNoFrameskip-v4', 'CrazyClimber-ram-v0', 'CrazyClimber-ramDeterministic-v0', 'CrazyClimber-ramNoFrameskip-v0', 'CrazyClimber-ram-v4', 'CrazyClimber-ramDeterministic-v4', 'CrazyClimber-ramNoFrameskip-v4', 'Defender-v0', 'DefenderDeterministic-v0', 'DefenderNoFrameskip-v0', 'Defender-v4', 'DefenderDeterministic-v4', 'DefenderNoFrameskip-v4', 'Defender-ram-v0', 'Defender-ramDeterministic-v0', 'Defender-ramNoFrameskip-v0', 'Defender-ram-v4', 'Defender-ramDeterministic-v4', 'Defender-ramNoFrameskip-v4', 'DemonAttack-v0', 'DemonAttackDeterministic-v0', 'DemonAttackNoFrameskip-v0', 'DemonAttack-v4', 'DemonAttackDeterministic-v4', 'DemonAttackNoFrameskip-v4', 'DemonAttack-ram-v0', 'DemonAttack-ramDeterministic-v0', 'DemonAttack-ramNoFrameskip-v0', 'DemonAttack-ram-v4', 'DemonAttack-ramDeterministic-v4', 'DemonAttack-ramNoFrameskip-v4', 'DoubleDunk-v0', 'DoubleDunkDeterministic-v0', 'DoubleDunkNoFrameskip-v0', 'DoubleDunk-v4', 'DoubleDunkDeterministic-v4', 'DoubleDunkNoFrameskip-v4', 'DoubleDunk-ram-v0', 'DoubleDunk-ramDeterministic-v0', 'DoubleDunk-ramNoFrameskip-v0', 'DoubleDunk-ram-v4', 'DoubleDunk-ramDeterministic-v4', 'DoubleDunk-ramNoFrameskip-v4', 'ElevatorAction-v0', 'ElevatorActionDeterministic-v0', 'ElevatorActionNoFrameskip-v0', 'ElevatorAction-v4', 'ElevatorActionDeterministic-v4', 'ElevatorActionNoFrameskip-v4', 'ElevatorAction-ram-v0', 'ElevatorAction-ramDeterministic-v0', 'ElevatorAction-ramNoFrameskip-v0', 'ElevatorAction-ram-v4', 'ElevatorAction-ramDeterministic-v4', 'ElevatorAction-ramNoFrameskip-v4', 'Enduro-v0', 'EnduroDeterministic-v0', 'EnduroNoFrameskip-v0', 'Enduro-v4', 'EnduroDeterministic-v4', 'EnduroNoFrameskip-v4', 'Enduro-ram-v0', 'Enduro-ramDeterministic-v0', 'Enduro-ramNoFrameskip-v0', 'Enduro-ram-v4', 'Enduro-ramDeterministic-v4', 'Enduro-ramNoFrameskip-v4', 'FishingDerby-v0', 'FishingDerbyDeterministic-v0', 'FishingDerbyNoFrameskip-v0', 'FishingDerby-v4', 'FishingDerbyDeterministic-v4', 'FishingDerbyNoFrameskip-v4', 'FishingDerby-ram-v0', 'FishingDerby-ramDeterministic-v0', 'FishingDerby-ramNoFrameskip-v0', 'FishingDerby-ram-v4', 'FishingDerby-ramDeterministic-v4', 'FishingDerby-ramNoFrameskip-v4', 'Freeway-v0', 'FreewayDeterministic-v0', 'FreewayNoFrameskip-v0', 'Freeway-v4', 'FreewayDeterministic-v4', 'FreewayNoFrameskip-v4', 'Freeway-ram-v0', 'Freeway-ramDeterministic-v0', 'Freeway-ramNoFrameskip-v0', 'Freeway-ram-v4', 'Freeway-ramDeterministic-v4', 'Freeway-ramNoFrameskip-v4', 'Frostbite-v0', 'FrostbiteDeterministic-v0', 'FrostbiteNoFrameskip-v0', 'Frostbite-v4', 'FrostbiteDeterministic-v4', 'FrostbiteNoFrameskip-v4', 'Frostbite-ram-v0', 'Frostbite-ramDeterministic-v0', 'Frostbite-ramNoFrameskip-v0', 'Frostbite-ram-v4', 'Frostbite-ramDeterministic-v4', 'Frostbite-ramNoFrameskip-v4', 'Gopher-v0', 'GopherDeterministic-v0', 'GopherNoFrameskip-v0', 'Gopher-v4', 'GopherDeterministic-v4', 'GopherNoFrameskip-v4', 'Gopher-ram-v0', 'Gopher-ramDeterministic-v0', 'Gopher-ramNoFrameskip-v0', 'Gopher-ram-v4', 'Gopher-ramDeterministic-v4', 'Gopher-ramNoFrameskip-v4', 'Gravitar-v0', 'GravitarDeterministic-v0', 'GravitarNoFrameskip-v0', 'Gravitar-v4', 'GravitarDeterministic-v4', 'GravitarNoFrameskip-v4', 'Gravitar-ram-v0', 'Gravitar-ramDeterministic-v0', 'Gravitar-ramNoFrameskip-v0', 'Gravitar-ram-v4', 'Gravitar-ramDeterministic-v4', 'Gravitar-ramNoFrameskip-v4', 'Hero-v0', 'HeroDeterministic-v0', 'HeroNoFrameskip-v0', 'Hero-v4', 'HeroDeterministic-v4', 'HeroNoFrameskip-v4', 'Hero-ram-v0', 'Hero-ramDeterministic-v0', 'Hero-ramNoFrameskip-v0', 'Hero-ram-v4', 'Hero-ramDeterministic-v4', 'Hero-ramNoFrameskip-v4', 'IceHockey-v0', 'IceHockeyDeterministic-v0', 'IceHockeyNoFrameskip-v0', 'IceHockey-v4', 'IceHockeyDeterministic-v4', 'IceHockeyNoFrameskip-v4', 'IceHockey-ram-v0', 'IceHockey-ramDeterministic-v0', 'IceHockey-ramNoFrameskip-v0', 'IceHockey-ram-v4', 'IceHockey-ramDeterministic-v4', 'IceHockey-ramNoFrameskip-v4', 'Jamesbond-v0', 'JamesbondDeterministic-v0', 'JamesbondNoFrameskip-v0', 'Jamesbond-v4', 'JamesbondDeterministic-v4', 'JamesbondNoFrameskip-v4', 'Jamesbond-ram-v0', 'Jamesbond-ramDeterministic-v0', 'Jamesbond-ramNoFrameskip-v0', 'Jamesbond-ram-v4', 'Jamesbond-ramDeterministic-v4', 'Jamesbond-ramNoFrameskip-v4', 'JourneyEscape-v0', 'JourneyEscapeDeterministic-v0', 'JourneyEscapeNoFrameskip-v0', 'JourneyEscape-v4', 'JourneyEscapeDeterministic-v4', 'JourneyEscapeNoFrameskip-v4', 'JourneyEscape-ram-v0', 'JourneyEscape-ramDeterministic-v0', 'JourneyEscape-ramNoFrameskip-v0', 'JourneyEscape-ram-v4', 'JourneyEscape-ramDeterministic-v4', 'JourneyEscape-ramNoFrameskip-v4', 'Kangaroo-v0', 'KangarooDeterministic-v0', 'KangarooNoFrameskip-v0', 'Kangaroo-v4', 'KangarooDeterministic-v4', 'KangarooNoFrameskip-v4', 'Kangaroo-ram-v0', 'Kangaroo-ramDeterministic-v0', 'Kangaroo-ramNoFrameskip-v0', 'Kangaroo-ram-v4', 'Kangaroo-ramDeterministic-v4', 'Kangaroo-ramNoFrameskip-v4', 'Krull-v0', 'KrullDeterministic-v0', 'KrullNoFrameskip-v0', 'Krull-v4', 'KrullDeterministic-v4', 'KrullNoFrameskip-v4', 'Krull-ram-v0', 'Krull-ramDeterministic-v0', 'Krull-ramNoFrameskip-v0', 'Krull-ram-v4', 'Krull-ramDeterministic-v4', 'Krull-ramNoFrameskip-v4', 'KungFuMaster-v0', 'KungFuMasterDeterministic-v0', 'KungFuMasterNoFrameskip-v0', 'KungFuMaster-v4', 'KungFuMasterDeterministic-v4', 'KungFuMasterNoFrameskip-v4', 'KungFuMaster-ram-v0', 'KungFuMaster-ramDeterministic-v0', 'KungFuMaster-ramNoFrameskip-v0', 'KungFuMaster-ram-v4', 'KungFuMaster-ramDeterministic-v4', 'KungFuMaster-ramNoFrameskip-v4', 'MontezumaRevenge-v0', 'MontezumaRevengeDeterministic-v0', 'MontezumaRevengeNoFrameskip-v0', 'MontezumaRevenge-v4', 'MontezumaRevengeDeterministic-v4', 'MontezumaRevengeNoFrameskip-v4', 'MontezumaRevenge-ram-v0', 'MontezumaRevenge-ramDeterministic-v0', 'MontezumaRevenge-ramNoFrameskip-v0', 'MontezumaRevenge-ram-v4', 'MontezumaRevenge-ramDeterministic-v4', 'MontezumaRevenge-ramNoFrameskip-v4', 'MsPacman-v0', 'MsPacmanDeterministic-v0', 'MsPacmanNoFrameskip-v0', 'MsPacman-v4', 'MsPacmanDeterministic-v4', 'MsPacmanNoFrameskip-v4', 'MsPacman-ram-v0', 'MsPacman-ramDeterministic-v0', 'MsPacman-ramNoFrameskip-v0', 'MsPacman-ram-v4', 'MsPacman-ramDeterministic-v4', 'MsPacman-ramNoFrameskip-v4', 'NameThisGame-v0', 'NameThisGameDeterministic-v0', 'NameThisGameNoFrameskip-v0', 'NameThisGame-v4', 'NameThisGameDeterministic-v4', 'NameThisGameNoFrameskip-v4', 'NameThisGame-ram-v0', 'NameThisGame-ramDeterministic-v0', 'NameThisGame-ramNoFrameskip-v0', 'NameThisGame-ram-v4', 'NameThisGame-ramDeterministic-v4', 'NameThisGame-ramNoFrameskip-v4', 'Phoenix-v0', 'PhoenixDeterministic-v0', 'PhoenixNoFrameskip-v0', 'Phoenix-v4', 'PhoenixDeterministic-v4', 'PhoenixNoFrameskip-v4', 'Phoenix-ram-v0', 'Phoenix-ramDeterministic-v0', 'Phoenix-ramNoFrameskip-v0', 'Phoenix-ram-v4', 'Phoenix-ramDeterministic-v4', 'Phoenix-ramNoFrameskip-v4', 'Pitfall-v0', 'PitfallDeterministic-v0', 'PitfallNoFrameskip-v0', 'Pitfall-v4', 'PitfallDeterministic-v4', 'PitfallNoFrameskip-v4', 'Pitfall-ram-v0', 'Pitfall-ramDeterministic-v0', 'Pitfall-ramNoFrameskip-v0', 'Pitfall-ram-v4', 'Pitfall-ramDeterministic-v4', 'Pitfall-ramNoFrameskip-v4', 'Pong-v0', 'PongDeterministic-v0', 'PongNoFrameskip-v0', 'Pong-v4', 'PongDeterministic-v4', 'PongNoFrameskip-v4', 'Pong-ram-v0', 'Pong-ramDeterministic-v0', 'Pong-ramNoFrameskip-v0', 'Pong-ram-v4', 'Pong-ramDeterministic-v4', 'Pong-ramNoFrameskip-v4', 'Pooyan-v0', 'PooyanDeterministic-v0', 'PooyanNoFrameskip-v0', 'Pooyan-v4', 'PooyanDeterministic-v4', 'PooyanNoFrameskip-v4', 'Pooyan-ram-v0', 'Pooyan-ramDeterministic-v0', 'Pooyan-ramNoFrameskip-v0', 'Pooyan-ram-v4', 'Pooyan-ramDeterministic-v4', 'Pooyan-ramNoFrameskip-v4', 'PrivateEye-v0', 'PrivateEyeDeterministic-v0', 'PrivateEyeNoFrameskip-v0', 'PrivateEye-v4', 'PrivateEyeDeterministic-v4', 'PrivateEyeNoFrameskip-v4', 'PrivateEye-ram-v0', 'PrivateEye-ramDeterministic-v0', 'PrivateEye-ramNoFrameskip-v0', 'PrivateEye-ram-v4', 'PrivateEye-ramDeterministic-v4', 'PrivateEye-ramNoFrameskip-v4', 'Qbert-v0', 'QbertDeterministic-v0', 'QbertNoFrameskip-v0', 'Qbert-v4', 'QbertDeterministic-v4', 'QbertNoFrameskip-v4', 'Qbert-ram-v0', 'Qbert-ramDeterministic-v0', 'Qbert-ramNoFrameskip-v0', 'Qbert-ram-v4', 'Qbert-ramDeterministic-v4', 'Qbert-ramNoFrameskip-v4', 'Riverraid-v0', 'RiverraidDeterministic-v0', 'RiverraidNoFrameskip-v0', 'Riverraid-v4', 'RiverraidDeterministic-v4', 'RiverraidNoFrameskip-v4', 'Riverraid-ram-v0', 'Riverraid-ramDeterministic-v0', 'Riverraid-ramNoFrameskip-v0', 'Riverraid-ram-v4', 'Riverraid-ramDeterministic-v4', 'Riverraid-ramNoFrameskip-v4', 'RoadRunner-v0', 'RoadRunnerDeterministic-v0', 'RoadRunnerNoFrameskip-v0', 'RoadRunner-v4', 'RoadRunnerDeterministic-v4', 'RoadRunnerNoFrameskip-v4', 'RoadRunner-ram-v0', 'RoadRunner-ramDeterministic-v0', 'RoadRunner-ramNoFrameskip-v0', 'RoadRunner-ram-v4', 'RoadRunner-ramDeterministic-v4', 'RoadRunner-ramNoFrameskip-v4', 'Robotank-v0', 'RobotankDeterministic-v0', 'RobotankNoFrameskip-v0', 'Robotank-v4', 'RobotankDeterministic-v4', 'RobotankNoFrameskip-v4', 'Robotank-ram-v0', 'Robotank-ramDeterministic-v0', 'Robotank-ramNoFrameskip-v0', 'Robotank-ram-v4', 'Robotank-ramDeterministic-v4', 'Robotank-ramNoFrameskip-v4', 'Seaquest-v0', 'SeaquestDeterministic-v0', 'SeaquestNoFrameskip-v0', 'Seaquest-v4', 'SeaquestDeterministic-v4', 'SeaquestNoFrameskip-v4', 'Seaquest-ram-v0', 'Seaquest-ramDeterministic-v0', 'Seaquest-ramNoFrameskip-v0', 'Seaquest-ram-v4', 'Seaquest-ramDeterministic-v4', 'Seaquest-ramNoFrameskip-v4', 'Skiing-v0', 'SkiingDeterministic-v0', 'SkiingNoFrameskip-v0', 'Skiing-v4', 'SkiingDeterministic-v4', 'SkiingNoFrameskip-v4', 'Skiing-ram-v0', 'Skiing-ramDeterministic-v0', 'Skiing-ramNoFrameskip-v0', 'Skiing-ram-v4', 'Skiing-ramDeterministic-v4', 'Skiing-ramNoFrameskip-v4', 'Solaris-v0', 'SolarisDeterministic-v0', 'SolarisNoFrameskip-v0', 'Solaris-v4', 'SolarisDeterministic-v4', 'SolarisNoFrameskip-v4', 'Solaris-ram-v0', 'Solaris-ramDeterministic-v0', 'Solaris-ramNoFrameskip-v0', 'Solaris-ram-v4', 'Solaris-ramDeterministic-v4', 'Solaris-ramNoFrameskip-v4', 'SpaceInvaders-v0', 'SpaceInvadersDeterministic-v0', 'SpaceInvadersNoFrameskip-v0', 'SpaceInvaders-v4', 'SpaceInvadersDeterministic-v4', 'SpaceInvadersNoFrameskip-v4', 'SpaceInvaders-ram-v0', 'SpaceInvaders-ramDeterministic-v0', 'SpaceInvaders-ramNoFrameskip-v0', 'SpaceInvaders-ram-v4', 'SpaceInvaders-ramDeterministic-v4', 'SpaceInvaders-ramNoFrameskip-v4', 'StarGunner-v0', 'StarGunnerDeterministic-v0', 'StarGunnerNoFrameskip-v0', 'StarGunner-v4', 'StarGunnerDeterministic-v4', 'StarGunnerNoFrameskip-v4', 'StarGunner-ram-v0', 'StarGunner-ramDeterministic-v0', 'StarGunner-ramNoFrameskip-v0', 'StarGunner-ram-v4', 'StarGunner-ramDeterministic-v4', 'StarGunner-ramNoFrameskip-v4', 'Tennis-v0', 'TennisDeterministic-v0', 'TennisNoFrameskip-v0', 'Tennis-v4', 'TennisDeterministic-v4', 'TennisNoFrameskip-v4', 'Tennis-ram-v0', 'Tennis-ramDeterministic-v0', 'Tennis-ramNoFrameskip-v0', 'Tennis-ram-v4', 'Tennis-ramDeterministic-v4', 'Tennis-ramNoFrameskip-v4', 'TimePilot-v0', 'TimePilotDeterministic-v0', 'TimePilotNoFrameskip-v0', 'TimePilot-v4', 'TimePilotDeterministic-v4', 'TimePilotNoFrameskip-v4', 'TimePilot-ram-v0', 'TimePilot-ramDeterministic-v0', 'TimePilot-ramNoFrameskip-v0', 'TimePilot-ram-v4', 'TimePilot-ramDeterministic-v4', 'TimePilot-ramNoFrameskip-v4', 'Tutankham-v0', 'TutankhamDeterministic-v0', 'TutankhamNoFrameskip-v0', 'Tutankham-v4', 'TutankhamDeterministic-v4', 'TutankhamNoFrameskip-v4', 'Tutankham-ram-v0', 'Tutankham-ramDeterministic-v0', 'Tutankham-ramNoFrameskip-v0', 'Tutankham-ram-v4', 'Tutankham-ramDeterministic-v4', 'Tutankham-ramNoFrameskip-v4', 'UpNDown-v0', 'UpNDownDeterministic-v0', 'UpNDownNoFrameskip-v0', 'UpNDown-v4', 'UpNDownDeterministic-v4', 'UpNDownNoFrameskip-v4', 'UpNDown-ram-v0', 'UpNDown-ramDeterministic-v0', 'UpNDown-ramNoFrameskip-v0', 'UpNDown-ram-v4', 'UpNDown-ramDeterministic-v4', 'UpNDown-ramNoFrameskip-v4', 'Venture-v0', 'VentureDeterministic-v0', 'VentureNoFrameskip-v0', 'Venture-v4', 'VentureDeterministic-v4', 'VentureNoFrameskip-v4', 'Venture-ram-v0', 'Venture-ramDeterministic-v0', 'Venture-ramNoFrameskip-v0', 'Venture-ram-v4', 'Venture-ramDeterministic-v4', 'Venture-ramNoFrameskip-v4', 'VideoPinball-v0', 'VideoPinballDeterministic-v0', 'VideoPinballNoFrameskip-v0', 'VideoPinball-v4', 'VideoPinballDeterministic-v4', 'VideoPinballNoFrameskip-v4', 'VideoPinball-ram-v0', 'VideoPinball-ramDeterministic-v0', 'VideoPinball-ramNoFrameskip-v0', 'VideoPinball-ram-v4', 'VideoPinball-ramDeterministic-v4', 'VideoPinball-ramNoFrameskip-v4', 'WizardOfWor-v0', 'WizardOfWorDeterministic-v0', 'WizardOfWorNoFrameskip-v0', 'WizardOfWor-v4', 'WizardOfWorDeterministic-v4', 'WizardOfWorNoFrameskip-v4', 'WizardOfWor-ram-v0', 'WizardOfWor-ramDeterministic-v0', 'WizardOfWor-ramNoFrameskip-v0', 'WizardOfWor-ram-v4', 'WizardOfWor-ramDeterministic-v4', 'WizardOfWor-ramNoFrameskip-v4', 'YarsRevenge-v0', 'YarsRevengeDeterministic-v0', 'YarsRevengeNoFrameskip-v0', 'YarsRevenge-v4', 'YarsRevengeDeterministic-v4', 'YarsRevengeNoFrameskip-v4', 'YarsRevenge-ram-v0', 'YarsRevenge-ramDeterministic-v0', 'YarsRevenge-ramNoFrameskip-v0', 'YarsRevenge-ram-v4', 'YarsRevenge-ramDeterministic-v4', 'YarsRevenge-ramNoFrameskip-v4', 'Zaxxon-v0', 'ZaxxonDeterministic-v0', 'ZaxxonNoFrameskip-v0', 'Zaxxon-v4', 'ZaxxonDeterministic-v4', 'ZaxxonNoFrameskip-v4', 'Zaxxon-ram-v0', 'Zaxxon-ramDeterministic-v0', 'Zaxxon-ramNoFrameskip-v0', 'Zaxxon-ram-v4', 'Zaxxon-ramDeterministic-v4', 'Zaxxon-ramNoFrameskip-v4', 'ALE/Adventure-v5', 'ALE/Adventure-ram-v5', 'ALE/AirRaid-v5', 'ALE/AirRaid-ram-v5', 'ALE/Alien-v5', 'ALE/Alien-ram-v5', 'ALE/Amidar-v5', 'ALE/Amidar-ram-v5', 'ALE/Assault-v5', 'ALE/Assault-ram-v5', 'ALE/Asterix-v5', 'ALE/Asterix-ram-v5', 'ALE/Asteroids-v5', 'ALE/Asteroids-ram-v5', 'ALE/Atlantis2-v5', 'ALE/Atlantis2-ram-v5', 'ALE/Atlantis-v5', 'ALE/Atlantis-ram-v5', 'ALE/Backgammon-v5', 'ALE/Backgammon-ram-v5', 'ALE/BankHeist-v5', 'ALE/BankHeist-ram-v5', 'ALE/BasicMath-v5', 'ALE/BasicMath-ram-v5', 'ALE/BattleZone-v5', 'ALE/BattleZone-ram-v5', 'ALE/BeamRider-v5', 'ALE/BeamRider-ram-v5', 'ALE/Berzerk-v5', 'ALE/Berzerk-ram-v5', 'ALE/Blackjack-v5', 'ALE/Blackjack-ram-v5', 'ALE/Bowling-v5', 'ALE/Bowling-ram-v5', 'ALE/Boxing-v5', 'ALE/Boxing-ram-v5', 'ALE/Breakout-v5', 'ALE/Breakout-ram-v5', 'ALE/Carnival-v5', 'ALE/Carnival-ram-v5', 'ALE/Casino-v5', 'ALE/Casino-ram-v5', 'ALE/Centipede-v5', 'ALE/Centipede-ram-v5', 'ALE/ChopperCommand-v5', 'ALE/ChopperCommand-ram-v5', 'ALE/Combat-v5', 'ALE/Combat-ram-v5', 'ALE/CrazyClimber-v5', 'ALE/CrazyClimber-ram-v5', 'ALE/Crossbow-v5', 'ALE/Crossbow-ram-v5', 'ALE/Darkchambers-v5', 'ALE/Darkchambers-ram-v5', 'ALE/Defender-v5', 'ALE/Defender-ram-v5', 'ALE/DemonAttack-v5', 'ALE/DemonAttack-ram-v5', 'ALE/DonkeyKong-v5', 'ALE/DonkeyKong-ram-v5', 'ALE/DoubleDunk-v5', 'ALE/DoubleDunk-ram-v5', 'ALE/Earthworld-v5', 'ALE/Earthworld-ram-v5', 'ALE/ElevatorAction-v5', 'ALE/ElevatorAction-ram-v5', 'ALE/Enduro-v5', 'ALE/Enduro-ram-v5', 'ALE/Entombed-v5', 'ALE/Entombed-ram-v5', 'ALE/Et-v5', 'ALE/Et-ram-v5', 'ALE/FishingDerby-v5', 'ALE/FishingDerby-ram-v5', 'ALE/FlagCapture-v5', 'ALE/FlagCapture-ram-v5', 'ALE/Freeway-v5', 'ALE/Freeway-ram-v5', 'ALE/Frogger-v5', 'ALE/Frogger-ram-v5', 'ALE/Frostbite-v5', 'ALE/Frostbite-ram-v5', 'ALE/Galaxian-v5', 'ALE/Galaxian-ram-v5', 'ALE/Gopher-v5', 'ALE/Gopher-ram-v5', 'ALE/Gravitar-v5', 'ALE/Gravitar-ram-v5', 'ALE/Hangman-v5', 'ALE/Hangman-ram-v5', 'ALE/HauntedHouse-v5', 'ALE/HauntedHouse-ram-v5', 'ALE/Hero-v5', 'ALE/Hero-ram-v5', 'ALE/HumanCannonball-v5', 'ALE/HumanCannonball-ram-v5', 'ALE/IceHockey-v5', 'ALE/IceHockey-ram-v5', 'ALE/Jamesbond-v5', 'ALE/Jamesbond-ram-v5', 'ALE/JourneyEscape-v5', 'ALE/JourneyEscape-ram-v5', 'ALE/Joust-v5', 'ALE/Joust-ram-v5', 'ALE/Kaboom-v5', 'ALE/Kaboom-ram-v5', 'ALE/Kangaroo-v5', 'ALE/Kangaroo-ram-v5', 'ALE/KeystoneKapers-v5', 'ALE/KeystoneKapers-ram-v5', 'ALE/KingKong-v5', 'ALE/KingKong-ram-v5', 'ALE/Klax-v5', 'ALE/Klax-ram-v5', 'ALE/Koolaid-v5', 'ALE/Koolaid-ram-v5', 'ALE/Krull-v5', 'ALE/Krull-ram-v5', 'ALE/KungFuMaster-v5', 'ALE/KungFuMaster-ram-v5', 'ALE/LaserGates-v5', 'ALE/LaserGates-ram-v5', 'ALE/LostLuggage-v5', 'ALE/LostLuggage-ram-v5', 'ALE/MarioBros-v5', 'ALE/MarioBros-ram-v5', 'ALE/MazeCraze-v5', 'ALE/MazeCraze-ram-v5', 'ALE/MiniatureGolf-v5', 'ALE/MiniatureGolf-ram-v5', 'ALE/MontezumaRevenge-v5', 'ALE/MontezumaRevenge-ram-v5', 'ALE/MrDo-v5', 'ALE/MrDo-ram-v5', 'ALE/MsPacman-v5', 'ALE/MsPacman-ram-v5', 'ALE/NameThisGame-v5', 'ALE/NameThisGame-ram-v5', 'ALE/Othello-v5', 'ALE/Othello-ram-v5', 'ALE/Pacman-v5', 'ALE/Pacman-ram-v5', 'ALE/Phoenix-v5', 'ALE/Phoenix-ram-v5', 'ALE/Pitfall2-v5', 'ALE/Pitfall2-ram-v5', 'ALE/Pitfall-v5', 'ALE/Pitfall-ram-v5', 'ALE/Pong-v5', 'ALE/Pong-ram-v5', 'ALE/Pooyan-v5', 'ALE/Pooyan-ram-v5', 'ALE/PrivateEye-v5', 'ALE/PrivateEye-ram-v5', 'ALE/Qbert-v5', 'ALE/Qbert-ram-v5', 'ALE/Riverraid-v5', 'ALE/Riverraid-ram-v5', 'ALE/RoadRunner-v5', 'ALE/RoadRunner-ram-v5', 'ALE/Robotank-v5', 'ALE/Robotank-ram-v5', 'ALE/Seaquest-v5', 'ALE/Seaquest-ram-v5', 'ALE/SirLancelot-v5', 'ALE/SirLancelot-ram-v5', 'ALE/Skiing-v5', 'ALE/Skiing-ram-v5', 'ALE/Solaris-v5', 'ALE/Solaris-ram-v5', 'ALE/SpaceInvaders-v5', 'ALE/SpaceInvaders-ram-v5', 'ALE/SpaceWar-v5', 'ALE/SpaceWar-ram-v5', 'ALE/StarGunner-v5', 'ALE/StarGunner-ram-v5', 'ALE/Superman-v5', 'ALE/Superman-ram-v5', 'ALE/Surround-v5', 'ALE/Surround-ram-v5', 'ALE/Tennis-v5', 'ALE/Tennis-ram-v5', 'ALE/Tetris-v5', 'ALE/Tetris-ram-v5', 'ALE/TicTacToe3D-v5', 'ALE/TicTacToe3D-ram-v5', 'ALE/TimePilot-v5', 'ALE/TimePilot-ram-v5', 'ALE/Trondead-v5', 'ALE/Trondead-ram-v5', 'ALE/Turmoil-v5', 'ALE/Turmoil-ram-v5', 'ALE/Tutankham-v5', 'ALE/Tutankham-ram-v5', 'ALE/UpNDown-v5', 'ALE/UpNDown-ram-v5', 'ALE/Venture-v5', 'ALE/Venture-ram-v5', 'ALE/VideoCheckers-v5', 'ALE/VideoCheckers-ram-v5', 'ALE/VideoChess-v5', 'ALE/VideoChess-ram-v5', 'ALE/VideoCube-v5', 'ALE/VideoCube-ram-v5', 'ALE/VideoPinball-v5', 'ALE/VideoPinball-ram-v5', 'ALE/Warlords-v5', 'ALE/Warlords-ram-v5', 'ALE/WizardOfWor-v5', 'ALE/WizardOfWor-ram-v5', 'ALE/WordZapper-v5', 'ALE/WordZapper-ram-v5', 'ALE/YarsRevenge-v5', 'ALE/YarsRevenge-ram-v5', 'ALE/Zaxxon-v5', 'ALE/Zaxxon-ram-v5'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import random\n",
    "from collections import deque\n",
    "import math\n",
    "from skimage.transform import resize\n",
    "\n",
    "#imprimir se a gpu esta disponivel\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "gym.envs.registration.registry.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),  # Input channels adjusted for RGB (3 or 4*3 if frame stacking).\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(input_shape), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def feature_size(self, input_shape):\n",
    "        return self.conv(torch.zeros(1, *input_shape)).view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (torch.FloatTensor(np.array(states)),\n",
    "                torch.LongTensor(np.array(actions)),\n",
    "                torch.FloatTensor(np.array(rewards)),\n",
    "                torch.FloatTensor(np.array(next_states)),\n",
    "                torch.FloatTensor(np.array(dones)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_shape, num_actions, learning_rate=0.00025, gamma=0.99, epsilon=1.0, epsilon_min=0.1, epsilon_decay=0.9999, batch_size=32, memory_size=50000, update_target_freq=100):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = DQN(state_shape, num_actions).to(self.device)\n",
    "        self.target_model = DQN(state_shape, num_actions).to(self.device)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())  # Initialize target network with the same weights\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.memory = ReplayBuffer(memory_size)\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.update_target_freq = update_target_freq  # Frequency to update target network\n",
    "        self.train_step = 0\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randrange(self.num_actions)  # Explore\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.model(state)\n",
    "            return q_values.argmax().item()  # Exploit\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return  # Not enough samples in memory\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.unsqueeze(1).to(self.device)  # Reshape for gather\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "\n",
    "        # Q-values for current states\n",
    "        q_values = self.model(states).gather(1, actions)\n",
    "\n",
    "        # Q-values for next states using target network\n",
    "        next_q_values = self.target_model(next_states).max(1)[0].unsqueeze(1)\n",
    "\n",
    "        # Target Q-values\n",
    "        target_q_values = rewards.unsqueeze(1) + self.gamma * next_q_values * (1 - dones.unsqueeze(1))\n",
    "\n",
    "        # Loss calculation\n",
    "        loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "        # Optimization step\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update train step and target network\n",
    "        self.train_step += 1\n",
    "        if self.train_step % self.update_target_freq == 0:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSNode:\n",
    "    def __init__(self, state, parent=None, action_taken=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.children = {}  # Dictionary of child nodes\n",
    "        self.visits = 0\n",
    "        self.value = 0\n",
    "\n",
    "class HybridDQNMCTS:\n",
    "    def __init__(self, dqn_agent, env, num_simulations=50, exploration_constant=1.4):\n",
    "        self.dqn_agent = dqn_agent\n",
    "        self.env = env\n",
    "        self.num_simulations = num_simulations  # Number of MCTS simulations\n",
    "        self.exploration_constant = exploration_constant  # C in UCT formula\n",
    "\n",
    "    def get_action(self, state):\n",
    "        root = MCTSNode(state)\n",
    "\n",
    "        for _ in range(self.num_simulations):\n",
    "            node = root\n",
    "            sim_env = gym.make(self.env.spec.id, render_mode=None) # Create a new simulation environment for each simulation\n",
    "            sim_state, _ = sim_env.reset()\n",
    "            sim_state = preprocess_state(sim_state)\n",
    "\n",
    "            # Selection\n",
    "            while node.children:\n",
    "                if len(node.children) < self.env.action_space.n:  # Check for unexplored actions\n",
    "                    action = self._expand(node, sim_state)\n",
    "                    if action is None: # Handle the edge case of fully expanded node.\n",
    "                        break\n",
    "\n",
    "                    sim_state, reward, terminated, truncated, _ = sim_env.step(action)\n",
    "                    sim_state = preprocess_state(sim_state)\n",
    "                    done = terminated or truncated\n",
    "                    child = MCTSNode(sim_state, node, action)\n",
    "                    node.children[action] = child\n",
    "                    node = child\n",
    "                    break\n",
    "                else:\n",
    "                    action = self._select_uct(node)\n",
    "                    sim_state, reward, terminated, truncated, _ = sim_env.step(action)\n",
    "                    sim_state = preprocess_state(sim_state)\n",
    "                    done = terminated or truncated\n",
    "                    node = node.children[action]\n",
    "\n",
    "            # Simulation (Rollout)\n",
    "            value = self._simulate(sim_state, sim_env)\n",
    "\n",
    "            # Backpropagation\n",
    "            while node:\n",
    "                node.visits += 1\n",
    "                node.value += value\n",
    "                node = node.parent\n",
    "            sim_env.close()\n",
    "\n",
    "        # Choose the action with the most visits\n",
    "        return max(root.children.items(), key=lambda x: x[1].visits)[0]\n",
    "\n",
    "    def _expand(self, node, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.dqn_agent.device)\n",
    "        q_values = self.dqn_agent.model(state_tensor)[0].cpu().detach().numpy()\n",
    "        untried_actions = [a for a in range(self.env.action_space.n) if a not in node.children]\n",
    "\n",
    "        if not untried_actions:\n",
    "            return None\n",
    "\n",
    "        # Prioritize actions based on DQN Q-values\n",
    "        action = max(untried_actions, key=lambda a: q_values[a])\n",
    "        return action\n",
    "\n",
    "    def _select_uct(self, node):\n",
    "        log_parent_visits = math.log(node.visits)\n",
    "\n",
    "        def uct_value(child):\n",
    "            exploitation = child.value / child.visits if child.visits > 0 else 0\n",
    "            exploration = math.sqrt(log_parent_visits / (child.visits + 1e-10))\n",
    "            return exploitation + self.exploration_constant * exploration\n",
    "\n",
    "        return max(node.children.items(), key=lambda x: uct_value(x[1]))[0]\n",
    "\n",
    "    def _simulate(self, state, sim_env):\n",
    "        value = 0\n",
    "        discount = 1.0\n",
    "        max_steps = 100  # Limit simulation steps\n",
    "        sim_state = state\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            state_tensor = torch.FloatTensor(sim_state).unsqueeze(0).to(self.dqn_agent.device)\n",
    "            q_values = self.dqn_agent.model(state_tensor)[0].cpu().detach().numpy()\n",
    "            action = np.argmax(q_values) # Use DQN for action selection during simulation\n",
    "            sim_state, reward, terminated, truncated, _ = sim_env.step(action)\n",
    "            sim_state = preprocess_state(sim_state)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            value += discount * reward\n",
    "            discount *= self.dqn_agent.gamma\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        return value\n",
    "\n",
    "class HybridAgent:\n",
    "    def __init__(self, state_shape, num_actions, env):\n",
    "        self.dqn_agent = DQNAgent(state_shape, num_actions)\n",
    "        self.mcts = HybridDQNMCTS(self.dqn_agent, env)\n",
    "        self.training_mode = True  # Toggle between training (DQN) and evaluation (MCTS)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if self.training_mode:\n",
    "            return self.dqn_agent.choose_action(state)  # DQN action selection during training\n",
    "        else:\n",
    "            return self.mcts.get_action(state)  # MCTS action selection during evaluation\n",
    "\n",
    "    def learn(self):\n",
    "        return self.dqn_agent.learn()  # DQN learning\n",
    "\n",
    "    def save(self, filepath):\n",
    "        torch.save(self.dqn_agent.model.state_dict(), filepath)\n",
    "\n",
    "    def load(self, filepath):\n",
    "        self.dqn_agent.model.load_state_dict(torch.load(filepath))\n",
    "        self.dqn_agent.target_model.load_state_dict(torch.load(filepath))  # Load into target model as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, agent, episodes=1000, save_freq=20):\n",
    "    try:\n",
    "        total_steps = 0\n",
    "        for episode in range(episodes):\n",
    "            state, _ = env.reset()\n",
    "            state = preprocess_state(state)\n",
    "\n",
    "            # Frame Stacking\n",
    "            stacked_frames = deque([state] * 4, maxlen=4)\n",
    "            state = np.transpose(state, (2, 0, 1))  # (H, W, C) -> (C, H, W)\n",
    "            stacked_frames = deque([state] * 4, maxlen=4)\n",
    "            state = np.concatenate(stacked_frames, axis=0)  # Concatenate along channel axis\n",
    "\n",
    "\n",
    "            episode_reward = 0\n",
    "            episode_steps = 0\n",
    "            start_time = time.time()\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action = agent.choose_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                next_state = preprocess_state(next_state)\n",
    "\n",
    "                # Frame Stacking - Update\n",
    "                next_state = np.transpose(next_state, (2, 0, 1))\n",
    "                stacked_frames.append(next_state)\n",
    "                next_state = np.concatenate(stacked_frames, axis=0)\n",
    "\n",
    "                done = terminated or truncated\n",
    "                agent.dqn_agent.memory.push(state, action, reward, next_state, done)  # Store transition in replay buffer\n",
    "                agent.learn() # Train the DQN\n",
    "\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                episode_steps += 1\n",
    "                total_steps += 1\n",
    "\n",
    "            episode_time = time.time() - start_time\n",
    "            steps_per_second = episode_steps / episode_time\n",
    "\n",
    "            print(f\"Episode {episode + 1}/{episodes}, Reward: {episode_reward}, Steps: {episode_steps}, Time: {episode_time:.2f}s, Steps/s: {steps_per_second:.2f}, Epsilon: {agent.dqn_agent.epsilon:.3f}\")\n",
    "\n",
    "            if (episode + 1) % save_freq == 0:\n",
    "                agent.save(f'dqn_model_episode_{episode + 1}.pth')\n",
    "        return agent\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        raise\n",
    "\n",
    "def preprocess_state(state):\n",
    "    \"\"\"Resize and normalize the state.\"\"\"\n",
    "    state = resize(state, (84, 84), anti_aliasing=True)\n",
    "    state = state.astype(np.float32)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rodando o treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rodando o treinamento com a escala RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 20, 20]          24,608\n",
      "              ReLU-2           [-1, 32, 20, 20]               0\n",
      "            Conv2d-3             [-1, 64, 9, 9]          32,832\n",
      "              ReLU-4             [-1, 64, 9, 9]               0\n",
      "            Conv2d-5             [-1, 64, 7, 7]          36,928\n",
      "              ReLU-6             [-1, 64, 7, 7]               0\n",
      "            Linear-7                  [-1, 512]       1,606,144\n",
      "              ReLU-8                  [-1, 512]               0\n",
      "            Linear-9                    [-1, 6]           3,078\n",
      "================================================================\n",
      "Total params: 1,703,590\n",
      "Trainable params: 1,703,590\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.32\n",
      "Forward/backward pass size (MB): 0.33\n",
      "Params size (MB): 6.50\n",
      "Estimated Total Size (MB): 7.15\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from ale_py import ALEInterface\n",
    "from torchsummary import summary\n",
    "\n",
    "ale = ALEInterface()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('ALE/SpaceInvaders-v5', render_mode=\"rgb_array\")\n",
    "    state_shape = (12, 84, 84)  # 4 frames * 3 channels (RGB)\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    hybrid_agent = HybridAgent(state_shape, num_actions, env)\n",
    "    # Resumo do modelo\n",
    "    summary(hybrid_agent.dqn_agent.model, state_shape, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    train_agent(env, hybrid_agent, episodes=10000)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rodando o modelo treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(input_shape), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def feature_size(self, input_shape):\n",
    "        return self.conv(torch.zeros(1, *input_shape)).view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class Agent:  # Simplified agent for inference\n",
    "    def __init__(self, state_shape, num_actions, epsilon=0.05):  # Add epsilon parameter\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = DQN(state_shape, num_actions).to(self.device)\n",
    "        self.epsilon = epsilon  # Store epsilon\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def load(self, filepath):\n",
    "        self.model.load_state_dict(torch.load(filepath, map_location=self.device))\n",
    "        self.model.eval()  # Set to evaluation mode\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.random() < self.epsilon:  # Epsilon-greedy\n",
    "            return random.randrange(self.num_actions)\n",
    "        else:\n",
    "            with torch.no_grad():  # No need to track gradients during inference\n",
    "                state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                q_values = self.model(state)\n",
    "                return q_values.argmax().item()\n",
    "\n",
    "def preprocess_state(state):\n",
    "    \"\"\"Resize and normalize the RGB state.\"\"\"\n",
    "    state = resize(state, (84, 84), anti_aliasing=True)  # Keep RGB channels\n",
    "    state = state.astype(np.float32)\n",
    "    return state\n",
    "# --- Inference Function ---\n",
    "\n",
    "def run_agent(env, agent, model_path, episodes=10):\n",
    "    agent.load(model_path)  # Load the trained model\n",
    "    total_rewards = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = preprocess_state(state)\n",
    "\n",
    "        # Frame Stacking (RGB) - Consistent with training\n",
    "        state = np.transpose(state, (2, 0, 1))  # (H, W, C) -> (C, H, W)\n",
    "        stacked_frames = deque([state] * 4, maxlen=4)\n",
    "        state = np.concatenate(stacked_frames, axis=0)\n",
    "\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = preprocess_state(next_state)\n",
    "\n",
    "            # Update frame stack (RGB) - Consistent\n",
    "            next_state = np.transpose(next_state, (2, 0, 1))  # (H, W, C) -> (C, H, W)\n",
    "            stacked_frames.append(next_state)\n",
    "            state = np.concatenate(stacked_frames, axis=0)\n",
    "\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            episode_steps += 1\n",
    "\n",
    "            env.render()  # Render the environment (human mode)\n",
    "\n",
    "\n",
    "        episode_time = time.time() - start_time\n",
    "        steps_per_second = episode_steps / episode_time\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Reward: {episode_reward}, Steps: {episode_steps}, Time: {episode_time:.2f}s, Steps/s: {steps_per_second:.2f}\")\n",
    "\n",
    "    env.close()\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    std_reward = np.std(total_rewards)\n",
    "    print(f\"\\nAverage Reward over {episodes} episodes: {avg_reward:.2f}\")\n",
    "    print(f\"Standard Deviation of Rewards: {std_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/5, Reward: 210.0, Steps: 650, Time: 24.03s, Steps/s: 27.05\n",
      "Episode 2/5, Reward: 155.0, Steps: 608, Time: 23.47s, Steps/s: 25.90\n",
      "Episode 3/5, Reward: 150.0, Steps: 543, Time: 22.76s, Steps/s: 23.85\n",
      "Episode 4/5, Reward: 410.0, Steps: 609, Time: 22.48s, Steps/s: 27.09\n",
      "Episode 5/5, Reward: 30.0, Steps: 303, Time: 10.92s, Steps/s: 27.75\n",
      "\n",
      "Average Reward over 5 episodes: 191.00\n",
      "Standard Deviation of Rewards: 124.27\n"
     ]
    }
   ],
   "source": [
    "from ale_py import ALEInterface\n",
    "\n",
    "ale = ALEInterface()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('ALE/SpaceInvaders-v5', render_mode=\"human\") #Human render mode.\n",
    "    state_shape = (12, 84, 84)  # 4 frames * 3 channels (RGB)\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    agent = Agent(state_shape, num_actions)\n",
    "    model_file = \"dqn_model_episode_900.pth\"  # Replace with your model's filename\n",
    "    run_agent(env, agent, model_file, episodes=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projetoiadataset",
   "language": "python",
   "name": "projetoiadataset"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
